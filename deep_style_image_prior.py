# -*- coding: utf-8 -*-
"""Copy of IMPR Deep Style Image Prior.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nm3Fa195W273fjWFZBAhoUhjV1dQmdro

# Important!
When initially opening the notebook there should be a text to the right of the "Help" menu saying "Changes will not be saved".
![WhatsApp Image 2021-01-02 at 22.02.20.jpeg](data:image/jpeg;base64,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)

To ensure you can make changes to the notebook save a copy of it to your own drive and work on that one. You can do that by going to "File" -> "Save a copy in Drive".

**Failing to do so will result in code loss!**

 **Note** Make sure you are the only one that has access to it!
"""

# Commented out IPython magic to ensure Python compatibility.
#@markdown #Image Processing - 67829. { display-mode: "form" }
#@markdown ##Exercise 5:  Deep Style Image Prior
#@markdown ##Due date: 20.03.2024 at 23:59
#@title{ display-mode: "form" }

#@markdown
#@markdown This exercise is a bit different than the rest of the exercises in the course.
#@markdown The submissions will be a PDF file with your answers and results to the exercise
#@markdown as well as some files so that we can verify the authenticity of your results.
#@markdown This notebook provides the basic code, but you do not need to adhere to some specific API
#@markdown and we will not be running unit tests on your code.
#@markdown We will however, be going over your code and running it manually.
#@markdown Moreover, we will be running tests to ensure the authenticity of your solution and detect plagiarism
#@markdown
#@markdown
#@markdown Before you start working on the exercise it is recommended that you review the lecture slides covering neural networks,
#@markdown
#@markdown
#@markdown **NOTE**: Neural networks are typically trained on GPUs, without GPUs training takes much longer.
#@markdown To enable GPU tranining click on "Runtime" -> "Change runtime type" -> "GPU" -> "SAVE".
#@markdown
#@markdown **NOTE**: A short guide on debugging your code using colab is availble [here](https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.06-Errors-and-Debugging.ipynb#scrollTo=qnIn-rWFqoww).

#@markdown But first, we have to download all of the dependencies and install them.
#@markdown Play this cell to download it and get everything ready. markdown This may take a few minutes.


!mkdir impr_ex5_resources
# %cd impr_ex5_resources
#!wget "https://www.cs.huji.ac.il/~impr/shape_predictor_68_face_landmarks.dat" -O shape_predictor_68_face_landmarks.dat
#!wget "https://www.cs.huji.ac.il/~impr/align_faces.py" -O align_faces.py
# !wget "https://www.cs.huji.ac.il/w~impr/stylegan2-ada-pytorch.tar" -O stylegan2-ada-pytorch.tar
!wget -v --max-redirect=5 --user-agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3"  "https://www.cs.huji.ac.il/w~impr/stylegan2-ada-pytorch.tar" -O stylegan2-ada-pytorch.tar
!tar -xvf stylegan2-ada-pytorch.tar
!rm -f stylegan2-ada-pytorch.tar

import sys
ROOT_PATH="/content/impr_ex5_resources/stylegan2-ada-pytorch"
sys.path.append(ROOT_PATH)


!pip install ninja
!pip install mediapy
CHECKPOINTS_PATH = "https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada/pretrained/ffhq.pkl"



import copy
import os
from time import perf_counter
import click
import imageio
import numpy as np
import PIL.Image
import torch
import torch.nn.functional as F

import dnnlib
import legacy
import numpy as np
from skimage.draw import line
from torch.nn.functional import conv2d,conv1d
import mediapy as media
from IPython.display import clear_output

"""# Mounting Google Drive"""

#@markdown **NOTE**: It is strongly advised you save your results to Google
#@markdown Drive as they will be deleted from Colab once it restarts.
#@markdown To connect Google Drive run this cell.
from google.colab import drive
drive.mount('/content/gdrive/')

"""Below is the root dir of your Google Drive. To choose the destenation of the dir to save and read from, create it in your Google Drive and add the relative path to the "GDRIVE_SAVE_REL_PATH" variable below."""

ROOT_GDRIVE_PATH="/content/gdrive/MyDrive/"
GDRIVE_SAVE_REL_PATH = "Ex5_impr/"
FULL_GDRIVE_SAVE_PATH = ROOT_GDRIVE_PATH + GDRIVE_SAVE_REL_PATH

"""# General Variables"""

GAUSSIAN_BLUR_DEGRADATION= 'GAUSSIAN_BLUR_DEGRADATION'
GRAYSCALE_DEGRADATION = 'GRAYSCALE_DEGRADATION'
INPAINTING_DEGRADATION = 'INPAINTING_DEGRADATION'
NO_DEGRADATION= 'NO_DEGRADATION'

"""# Image Alignment"""

# The align_faces.py script takes in an input image path, an output image path, and a dat file path. The dat file is already downloaded for you, so leave it as it is.
# It is advised that you save the files to google drive as restarting Colab will erase them.
!python "$ROOT_PATH/align_faces/align_faces.py" '/content/gdrive/MyDrive/Ex5_impr/face.jpeg' '/content/gdrive/MyDrive/Ex5_impr/Talor_alignment.jpeg' "$ROOT_PATH/align_faces/shape_predictor_68_face_landmarks.dat"

"""# Degradation Functions"""
from scipy.signal import convolve2d
import torch
import numpy as np
import cv2
from skimage.color import rgb2gray
import torch.nn.functional as F

def grayscale_degradation(image):
  """
  Converts an RGB image tensor to grayscale.

  Parameters:
  *image (torch.Tensor): The input RGB image tensor with shape
  (batch_size, channels, height, width).

  Returns:
  *grayscale_image (torch.Tensor): The resulting grayscale image tensor with shape
  (batch_size, channels, height, width).
  """
  grayscale_image = (0.2989 * image[:, 0, :, :] + 0.5870 * image[:, 1, :, :] + 0.1140 * image[:, 2, :, :]).unsqueeze(1)
  # Repeated across channels to match the original RGB format
  grayscale_image = grayscale_image.repeat(1, 3, 1, 1)
  return grayscale_image


def inpainting_degradation(image, G):
  """
  Simulates image degradation for inpainting tasks based on a given mask.

  Parameters:
  - image (torch.Tensor): The original image tensor with shape (batch_size, channels, height, width).
  - mask (torch.Tensor): The mask tensor indicating areas to be inpainted, with values of 0 or 1.

  Returns:
  - result (torch.Tensor): The degraded image tensor with areas outside the mask preserved.
  """
  mask_target_pil = PIL.Image.open('/content/gdrive/MyDrive/Ex5_impr/fei_fei_li_inpainting_mask.png').convert('L')
  # Convert the PIL image to a numpy array
  mask_np = np.array(mask_target_pil)
  # Convert the numpy array to a PyTorch tensor
  mask = torch.from_numpy(mask_np)
  # Unsqueeze the tensor twice along the batch dimension to match the expected input format
  mask = mask.unsqueeze(0)
  mask = mask.unsqueeze(0)
  # Moves the tensor to the GPU device
  mask = mask.cuda()
  # Normalizes the pixel values of the mask tensor
  mask = mask / 255.
  mask = torch.nn.functional.interpolate(mask, size=(G.img_resolution, G.img_resolution), mode='bilinear')
  # Applies the inpainting degradation to the synthesized images
  result = image * mask + (1-mask)
  return result

"""# GAN Inversion"""

import torch
import matplotlib.pyplot as plt
def run_latent_optimization(outdir,
    degradation_mode,
    G,
    imgs_to_disply_dict,
    target: torch.Tensor, # [C,H,W] and dynamic range [0,255], W & H must match G output resolution
    *,
    num_steps                  = 1000,
    w_avg_samples              = 10000,
    initial_learning_rate      = 0.1,
    initial_noise_factor       = 0.05,
    lr_rampdown_length         = 0.25,
    lr_rampup_length           = 0.05,
    noise_ramp_length          = 0.75,
    regularize_noise_weight    = 1e5,
    latent_dist_reg_weight     = 0.01,
    device: torch.device

):
    assert target.shape == (G.img_channels, G.img_resolution, G.img_resolution)

    G = copy.deepcopy(G).eval().requires_grad_(False).to(device) # type: ignore

    # Compute w stats.
    print(f'Computing W midpoint and stddev using {w_avg_samples} samples...')
    z_samples = np.random.RandomState(123).randn(w_avg_samples, G.z_dim)
    w_samples = G.mapping(torch.from_numpy(z_samples).to(device), None)  # [N, L, C]
    w_samples = w_samples.cpu().numpy().astype(np.float32)
    w_avg = np.mean(w_samples, axis=0, keepdims=True)      # [1, 18, C]
    w_avg_original = torch.from_numpy(w_avg).to(device).float()
    w_std = (np.sum((w_samples - w_avg) ** 2) / w_avg_samples) ** 0.5

    # Setup noise inputs.
    noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }

    # Load VGG16 feature detector.
    url = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metrics/vgg16.pt'
    with dnnlib.util.open_url(url) as f:
        vgg16 = torch.jit.load(f).eval().to(device)

    # Features for target image.
    target_images = target.unsqueeze(0).to(device).to(torch.float32)

    if target_images.shape[2] > 256:
        target_images = F.interpolate(target_images, size=(256, 256), mode='area')
    target_features = vgg16(target_images, resize_images=False, return_lpips=True)

    w_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True)
    w_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)
    optimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)
    total_loss = []
    # Init noise.
    for buf in noise_bufs.values():
        buf[:] = torch.randn_like(buf)
        buf.requires_grad = True

    for step in range(num_steps):
        # Learning rate schedule.
        t = step / num_steps
        w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t / noise_ramp_length) ** 2
        lr_ramp = min(1.0, (1.0 - t) / lr_rampdown_length)
        lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)
        lr_ramp = lr_ramp * min(1.0, t / lr_rampup_length)
        lr = initial_learning_rate * lr_ramp
        for param_group in optimizer.param_groups:
            param_group['lr'] = lr

        # Synth image from opt_w
        w_noise = torch.randn_like(w_opt) * w_noise_scale
        ws = w_opt + w_noise
        synth_images = G.synthesis(ws, noise_mode='const')

        # Prep to save synth image
        synth_image_save = (synth_images + 1) * (255/2)
        synth_image_save = synth_image_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()

        if  degradation_mode == INPAINTING_DEGRADATION:
            synth_images = inpainting_degradation(synth_images, G)
        elif degradation_mode == GRAYSCALE_DEGRADATION:
            synth_images  = grayscale_degradation(synth_images)
        elif degradation_mode == GAUSSIAN_BLUR_DEGRADATION:
          pass

        # Prep to save and show images
        synth_image_degraded_save = (synth_images + 1) * (255/2)
        synth_image_degraded_save = synth_image_degraded_save.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()

        if step % 20 == 0:
          imgs_to_disply_dict["Generated Image"]=synth_image_save
          imgs_to_disply_dict["Generated Degraded Image"]=synth_image_degraded_save
          clear_output(wait=True)
          media.show_images(imgs_to_disply_dict,height=256)
        if step % 100 == 0:
          PIL.Image.fromarray(synth_image_save, 'RGB').save(f'{outdir}/intermidiate_%d_not_degraded.png'%step)
          PIL.Image.fromarray(synth_image_degraded_save, 'RGB').save(f'{outdir}/intermidiate_%d_degraded.png'%step)


        # Noise regularization.
        reg_loss = 0.0
        for v in noise_bufs.values():
            noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()
            while True:
                reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2
                reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2
                if noise.shape[2] <= 8:
                    break
                noise = F.avg_pool2d(noise, kernel_size=2)

        # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.
        synth_images = (synth_images + 1) * (255/2)
        if synth_images.shape[2] > 256:
            synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')

        # Features for synth images.
        synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)

        # Compute loss
        percep_loss = (target_features - synth_features).square().sum()
        latent_dist_reg = F.l1_loss(w_avg_original, w_opt)
        loss = percep_loss + reg_loss * regularize_noise_weight  + latent_dist_reg_weight * latent_dist_reg
        total_loss.append(loss.item())



        # Step
        optimizer.zero_grad(set_to_none=True)
        loss.backward()
        optimizer.step()

        print(f'step {step+1:>4d}/{num_steps}: percep_loss {percep_loss:<4.2f} latent_dist_reg {latent_dist_reg:<4.2f} loss {float(loss):<5.2f}' )

        # Save inverted latent for each optimization step.
        w_out[step] = w_opt.detach()[0]

        # Normalize noise.
        with torch.no_grad():
            for buf in noise_bufs.values():
                buf -= buf.mean()
                buf *= buf.square().mean().rsqrt()
    # Plot the logarithm linear loss array
    plt.plot(np.log(total_loss))
    plt.xlabel('iteration')
    plt.ylabel('Log Loss')
    plt.savefig(f'{outdir}/log_optimization_loss.png')
    plt.show()
    return w_out

import numpy as np
import torch.nn.functional as F
def invert_image(degradation_mode,
                   target_fname ='/content/gdrive/MyDrive/Ex5_impr/Talor_alignment.jpeg',
                   outdir='/content/gdrive/MyDrive/Ex5_impr/out/',
                   seed=303,
                   num_steps=1000,
                   latent_dist_reg_weight=0.):
    np.random.seed(seed)
    torch.manual_seed(seed)

    # Load networks.
    print('Loading networks from "%s"...' % CHECKPOINTS_PATH)
    device = torch.device('cuda')
    with dnnlib.util.open_url(CHECKPOINTS_PATH) as fp:
        networks = legacy.load_network_pkl(fp)
        G = networks['G_ema'].requires_grad_(False).to(device)


    # Load target image.
    if not os.path.exists(outdir):
      os.makedirs(outdir)
    target_pil = PIL.Image.open(target_fname).convert('RGB')
    w, h = target_pil.size
    s = min(w, h)
    target_pil = target_pil.crop(((w - s) // 2, (h - s) // 2, (w + s) // 2, (h + s) // 2))
    target_pil = target_pil.resize((G.img_resolution, G.img_resolution), PIL.Image.LANCZOS)
    target_uint8 = np.array(target_pil, dtype=np.uint8)
    target=torch.tensor([target_uint8.transpose([2, 0, 1])], device=device)
    target_images = target[0].unsqueeze(0).to(device).to(torch.float32)

    if  degradation_mode == INPAINTING_DEGRADATION:
        # Applies the inpainting degradation to the synthesized images
        target = inpainting_degradation(target_images, G)
    elif degradation_mode == GRAYSCALE_DEGRADATION:
        target = grayscale_degradation(target_images)
    elif degradation_mode == GAUSSIAN_BLUR_DEGRADATION:
        pass
    #Save target image
    target_to_save = target.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()
    PIL.Image.fromarray(target_to_save, 'RGB').save(f'{outdir}/original_degraded_image.png')
    imgs_to_disply_dict = {
        "Original Image":target_uint8,
        "Original Degraded Image":target_to_save,
              }

    # Run latent optimization
    start_time = perf_counter()
    optimization_steps = run_latent_optimization(
        outdir,
        degradation_mode,
        G,
        imgs_to_disply_dict,
        target[0],
        num_steps=num_steps,
        device=device,
        latent_dist_reg_weight=latent_dist_reg_weight
    )

    print (f'Elapsed: {(perf_counter()-start_time):.1f} s')
    os.makedirs(outdir, exist_ok=True)

    # Save final inverted image and latent vector.
    inverted_latent = optimization_steps[-1]
    synth_image = G.synthesis(inverted_latent.unsqueeze(0), noise_mode='const')
    synth_image = (synth_image + 1) * (255/2)
    synth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()
    PIL.Image.fromarray(synth_image, 'RGB').save(f'{outdir}/final_inverted_image.png')
    np.savez(f'{outdir}/inverted_latent.npz', latent=inverted_latent.unsqueeze(0).cpu().numpy())

invert_image(GRAYSCALE_DEGRADATION)